@article{Carpenter2017,
abstract = {{\textcopyright} 2017 American Statistical Association. All rights reserved. Stan is a probabilistic programming language for specifying statistical models. A Stan program imperatively defines a log probability function over parameters conditioned on specified data and constants. As of version 2.14.0, Stan provides full Bayesian inference for continuous-variable models through Markov chain Monte Carlo methods such as the No-U-Turn sampler, an adaptive form of Hamiltonian Monte Carlo sampling. Penalized maximum likelihood estimates are calculated using optimization methods such as the limited memory Broyden-Fletcher-Goldfarb-Shanno algorithm. Stan is also a platform for computing log densities and their gradients and Hessians, which can be used in alternative algorithms such as variational Bayes, expectation propagation, and marginal inference using approximate integration. To this end, Stan is set up so that the densities, gradients, and Hessians, along with intermediate quantities of the algorithm such as acceptance probabilities, are easily accessible. Stan can be called from the command line using the cmdstan package, through R using the rstan package, and through Python using the pystan package. All three interfaces support sampling and optimization-based inference with diagnostics and posterior analysis. rstan and pystan also provide access to log probabilities, gradients, Hessians, parameter transforms, and specialized plotting.},
author = {Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew D. and Lee, Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker, Marcus and Guo, Jiqiang and Li, Peter and Riddell, Allen},
doi = {10.18637/jss.v076.i01},
issn = {1548-7660},
journal = {Journal of Statistical Software},
number = {1},
title = {{{\textless}i{\textgreater}Stan{\textless}/i{\textgreater} : A Probabilistic Programming Language}},
url = {http://www.jstatsoft.org/v76/i01/},
volume = {76},
year = {2017}
}

@article{Crawford1998,
author = {Crawford, John R. and Howell, David C},
doi = {10.1076/clin.12.4.482.7241},
file = {::},
journal = {The Clinical Neuropsychologist},
mendeley-groups = {statistica},
number = {4},
pages = {482--486},
title = {{Comparing an Individual ' s Test Score Against Norms Derived from Small Samples}},
volume = {12},
year = {1998}
}

@article{Crawford2010,
author = {Crawford, John R. and Garthwaite, Paul H and Porter, Sara and Point, Sara Porter},
doi = {10.1080/02643294.2010.513967},
title = {{Point and interval estimates of effect sizes for the case-controls design in neuropsychology : Rationale , methods , implementations , and proposed reporting standards Point and interval estimates of effect sizes for the case-controls design in neuropsych}},
volume = {3294},
year = {2010}
}
@article{Crawford2005,
abstract = {The conventional criteria for a classical dissociation in single-case studies require that a patient be impaired on one task and within normal limits on another. J. R. Crawford and P. H. Garthwaite (2005) proposed an additional criterion, namely, that the patient's (standardized) difference on the two tasks should differ from the distribution of differences in controls. Monte Carlo simulation was used to evaluate these criteria. When Type I errors were defined as falsely concluding that a control case exhibited a dissociation, error rates were high for the conventional criteria but low for Crawford and Garthwaite's criteria. When Type I error rates were defined as falsely concluding that a patient with equivalent deficits on the two tasks exhibited a dissociation, error rates were very high for the conventional criteria but acceptable for the latter criteria. These latter criteria were robust in the face of nonnormal control data. The power to detect classical dissociations was studied.},
author = {Crawford, John R. and Garthwaite, Paul H.},
doi = {10.1037/0894-4105.19.5.664},
isbn = {0894-4105$\backslash$n1931-1559},
issn = {08944105},
journal = {Neuropsychology},
keywords = {Dissociations,Single-case studies,Statistical methods},
mendeley-groups = {statistica},
number = {5},
pages = {664--678},
pmid = {16187885},
title = {{Evaluation of criteria for classical dissociations in single-case studies by Monte Carlo simulation}},
volume = {19},
year = {2005}
}

@misc{scandola_romano_2020,
 title={Bayesian Multilevel Single Case Models using 'Stan'. A new tool to study single cases in Neuropsychology},
 url={psyarxiv.com/sajdq},
 DOI={10.31234/osf.io/sajdq},
 publisher={PsyArXiv},
 author={Scandola, Michele and Romano, Daniele},
 year={2020},
 month={Aug}
}
@article{Ottoboni2005,
abstract = {The authors used a modified Simon task (J. R. Simon, 1969) to assess the automatic recognition of handedness. Participants responded to the color of a circle in the center of the photograph of a right or a left hand, displayed in the center of the computer screen. A regular Simon effect was found for back views, whereas a reverse Simon effect was found for palm views. This pattern of results was found when the forearm was present (Experiment 2), when 1 finger at a time was rendered invisible (Experiment 3), and when the hands were connected to a body image (Experiment 4). When the hands were cut at the wrist, no effect emerged (Experiment 1). These findings suggest an automatic encoding of the relative position of the hand in relation to the body (imaginary or real) rather than the automatic encoding of handedness.},
author = {Ottoboni, Giovanni and Tessari, Alessia and Cubelli, Roberto and Umilt{\`{a}}, Carlo},
doi = {10.1037/0096-1523.31.4.778},
isbn = {0096-1523},
issn = {1939-1277},
journal = {Journal of Experimental Psychology: Human Perception and Performance},
keywords = {automatic processing,body,handedness recognition,sidedness recognition,simon effect},
mendeley-groups = {sidedness effect},
number = {4},
pages = {778--789},
pmid = {16131249},
title = {{Is handedness recognition automatic? A study using a Simon-like paradigm.}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0096-1523.31.4.778},
volume = {31},
year = {2005}
}
@article{Tessari2012,
abstract = {There is evidence suggesting that viewing hands triggers automatic access to the Body Structural Description, a visual-spatial representation of human body parts configuration. Hands, however, have a special representational status within the brain because of their significance for action and cognition. We tested whether feet, less important in gestural and object-directed action, would similarly show automatic access to the Body Structural Description. Positive evidence of that would be finding a Sidedness effect (Ottoboni et al. J Exp Psychol Hum Percept Perform 31:778-789, 2005), a Simon-like paradigm previously used to study automatic hand recognition. This effect demonstrates that processing hands generates spatial codes corresponding to the side of the body on which the hand would be located within the Body Structural Description map. Feet were shown with toes pointing upwards (Experiment 1), without any connection to the ankle and the leg (Experiment 2) and with toes pointing downwards (Experiment 3). Results revealed a Sidedness effect in both Experiments 1 and 3: spatial compatibility occurred according to the side of the body that each foot would assume within the Body Structural Description. In Experiment 2, as already found in stimuli similarly featured, no effect emerged, due to the lack of the necessary anatomical links connecting the foot to a body. Results suggest that body parts with variable degrees of significance for action and cognition can access automatically the Body Structural Description hence reinforcing the hypothesis of its pure visuo-spatial nature.},
author = {Tessari, Alessia and Ottoboni, Giovanni and Baroni, Giulia and Symes, Ed and Nicoletti, Roberto},
doi = {10.1007/s00221-012-3045-4},
isbn = {0014-4819},
issn = {00144819},
journal = {Experimental Brain Research},
keywords = {Body parts coding,Body representation,Feet representation,Sidedness Effect},
mendeley-groups = {sidedness effect},
number = {4},
pages = {515--525},
pmid = {22402752},
title = {{Is access to the body structural description sensitive to a body part's significance for action and cognition? A study of the sidedness effect using feet}},
volume = {218},
year = {2012}
}
@article{Gelman2013,
abstract = {Abstract: Posterior predictive p-values do not in general have uniform distributions under the null hypothesis (except in the special case of ancillary test variables) but instead tend to have distributions more concentrated near 0.5. From different perspectives, such ...$\backslash$n},
archivePrefix = {arXiv},
arxivId = {math.PR/0000000},
author = {Gelman, Andrew},
doi = {10.1214/13-EJS854},
eprint = {0000000},
issn = {1935-7524},
journal = {Electronic Journal of Statistics},
keywords = {and phrases,bayesian inference,model checking,p-value,posterior,predictive check,u-value},
mendeley-groups = {statistica},
pages = {1--8},
primaryClass = {math.PR},
title = {{Understanding posterior p-values}},
year = {2013}
}

@article{Gelman1992,
abstract = {The Gibbs sampler, the algorithm of Metropolis and similar iterative simulation methods are potentially very helpful for summarizing multivariate distributions. Used naively, however, iterative simulation can give misleading answers. Our methods are simple and generally applicable to the output of any iterative simulation; they are designed for researchers primarily interested in the science underlying the data and models they are analyzing, rather than for researchers interested in the probability theory underlying the iterative simulations themselves. Our recommended strategy is to use several independent sequences, with starting points sampled from an overdispersed distribution. At each step of the iterative simulation, we obtain, for each univariate estimand of interest, a distributional estimate and an estimate of how much sharper the distributional estimate might become if the simulations were contin- ued indefinitely. Because our focus is on applied inference for Bayesian posterior distributions in real problems, which often tend toward normal- ity after transformations and marginalization, we derive our results as normal-theory approximations to exact Bayesian inference, conditional on the observed simulations. The methods are illustrated on a random- effects mixture model applied to experimental measurements of reaction times of normal and schizophrenic patients.},
author = {Gelman, Andrew and Rubin, Donald B},
file = {::},
journal = {Statistical Science},
keywords = {algorithm,and phrases,bayesian inference,convergence of stochastic,ecm,em,gibbs sampler,importance sampling,metropolis,multiple imputation,processes,random-effects model,sir},
mendeley-groups = {statistica},
number = {4},
pages = {457--472},
title = {{Inference from Iterative Simulation Using Multiple Sequences}},
volume = {7},
year = {1992}
}

@book{gelman2013bayesian,
  title={Bayesian data analysis},
  author={Gelman, Andrew and Carlin, John B and Stern, Hal S and Dunson, David B and Vehtari, Aki and Rubin, Donald B},
  year={2013},
  publisher={CRC press}
}
@incollection{Raftery1995,
address = {Cambridge},
author = {Raftery, Adrian E},
booktitle = {Sociological Methodology},
editor = {Marsden, P V},
file = {:home/michele/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Raftery - 1995 - Bayesian Model Selection in Social Research.pdf:pdf},
number = {1995},
pages = {111--163},
publisher = {Blackwells},
title = {{Bayesian Model Selection in Social Research}},
volume = {25},
year = {1995}
}

@book{Kruschke2014,
abstract = {There is an explosion of interest in Bayesian statistics, primarily because recently created computational methods have finally made Bayesian analysis obtainable to a wide audience. Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan provides an accessible approach to Bayesian data analysis, as material is explained clearly with concrete examples. The book begins with the basics, including essential concepts of probability and random sampling, and gradually progresses to advanced hierarchical modeling methods for realistic data. Included are step-by-step instructions on how to conduct Bayesian data analyses in the popular and free software R and WinBugs. This book is intended for first-year graduate students or advanced undergraduates. It provides a bridge between undergraduate training and modern Bayesian methods for data analysis, which is becoming the accepted research standard. Knowledge of algebra and basic calculus is a prerequisite.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Kruschke, John K.},
booktitle = {Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan, Second Edition},
doi = {10.1016/B978-0-12-405888-0.09999-2},
edition = {2},
eprint = {arXiv:1011.1669v3},
file = {::},
isbn = {9780124058880},
issn = {1385-4046},
mendeley-groups = {statistica},
pages = {1--759},
pmid = {15003161},
publisher = {Elsevier Inc.},
title = {{Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan, second edition}},
url = {http://dx.doi.org/10.1016/B978-0-12-405888-0.09999-2},
year = {2014}
}
